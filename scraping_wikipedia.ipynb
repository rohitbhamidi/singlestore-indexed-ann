{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "\n",
    "import getpass\n",
    "from openai import OpenAI\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import sqlalchemy as sa\n",
    "from s2_openai_info import API_KEY, USERNAME, PASSWORD, CONN_STR, PORT, DATABASE, EMBEDDING_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get('https://en.wikipedia.org/wiki/Wikipedia:Good_articles/Video_games')\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "links = soup.find_all('a')\n",
    "urls = [link.get('href') for link in links if link.get('href') is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_patterns = [\n",
    "    '#',  # Anchor links\n",
    "    '/wiki/Main_Page',\n",
    "    '/wiki/Wikipedia:',\n",
    "    '/wiki/Portal:',\n",
    "    '/wiki/Special:',\n",
    "    '/wiki/Help:',\n",
    "    '//en.wikipedia.org/wiki/Wikipedia:',\n",
    "    'https://donate.wikimedia.org/wiki/Special:',\n",
    "    '/w/index.php?title=Special:',\n",
    "    '/wiki/Special:My',\n",
    "    'https://www.wikidata.org/wiki/Special:',\n",
    "    '/w/index.php?title=',\n",
    "    '/wiki/File:',\n",
    "    '/wiki/Category:',\n",
    "    '/wiki/Template:',\n",
    "    '/wiki/Wikipedia_talk:',\n",
    "    '/wiki/User:',\n",
    "]\n",
    "\n",
    "filtered_urls = ['https://en.wikipedia.org' + url for url in urls if not any(pattern in url for pattern in excluded_patterns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=API_KEY)\n",
    "engine = sa.create_engine(f'mysql+pymysql://{USERNAME}:{PASSWORD}@{CONN_STR}:{PORT}/{DATABASE}')\n",
    "conn = engine.connect()\n",
    "print('Connected to SingleStore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table():\n",
    "    '''Creates the table in SingleStore'''\n",
    "    conn.execute(sa.text('''DROP TABLE IF EXISTS wiki_scrape;'''))\n",
    "    conn.execute(sa.text('''\n",
    "    CREATE TABLE wiki_scrape(\n",
    "        id BIGINT AUTO_INCREMENT PRIMARY KEY,\n",
    "        url VARCHAR(255),\n",
    "        paragraph TEXT,\n",
    "        embedding VECTOR(1536, F32) NOT NULL,\n",
    "        FULLTEXT (paragraph),\n",
    "        VECTOR INDEX (embedding) INDEX_OPTIONS '{\"index_type\":\"IVF_PQ\"}'\n",
    "    );\n",
    "    '''))\n",
    "    print('Table created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''cleans the text of a wiki page'''\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    text = re.sub(r'\\(.*?\\)', '', text)\n",
    "    text = re.sub(r'\\<.*?\\>', '', text)\n",
    "    text = re.sub(r'\\n', '', text)\n",
    "    text = re.sub(r'\\t', '', text)\n",
    "    text = re.sub(r'\\s\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def get_text(url):\n",
    "    '''Gets the text from a wiki page and returns it as a string.'''\n",
    "    try:\n",
    "        page = requests.get(url)\n",
    "        page.raise_for_status()  # Raises an HTTPError if the HTTP request returned an unsuccessful status code\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        paragraphs = soup.find_all('p')\n",
    "        cleaned_paragraphs = [clean_text(p.text) for p in paragraphs if p.text.strip()]\n",
    "        return cleaned_paragraphs\n",
    "    except requests.RequestException as e:\n",
    "        # print(f\"Error fetching URL {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_embedding(text, model=EMBEDDING_MODEL):\n",
    "    '''Generates the OpenAI embedding from an input `text`.'''\n",
    "    try:\n",
    "        if isinstance(text, str):\n",
    "            response = client.embeddings.create(input=[text], model=model)\n",
    "            embedding = response.data[0].embedding\n",
    "            # return np.array(embedding).tobytes()\n",
    "            return json.dumps(embedding)\n",
    "        else:\n",
    "            # print(f\"Invalid input: {text}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        # print(f\"Error generating embedding: {e}\")\n",
    "        return None\n",
    "\n",
    "def text_embedding_df(url):\n",
    "    '''Creates a dataframe of the text from a wiki page and the OpenAI embeddings of that text'''\n",
    "    text = get_text(url)\n",
    "    embeddings = [get_embedding(t) for t in text]\n",
    "    df = pd.DataFrame({'paragraph': text, 'embedding': embeddings})\n",
    "    return df\n",
    "\n",
    "def scrape_wiki(url_list, table_name, engine):\n",
    "    '''Pushes a dataframe to a SingleStore table'''\n",
    "    for url in tqdm(url_list):\n",
    "        dataframe = text_embedding_df(url)\n",
    "        dataframe['url'] = url \n",
    "        dataframe = dataframe[['url', 'paragraph', 'embedding']]\n",
    "        dataframe = dataframe[dataframe['embedding'].notna()]\n",
    "        dataframe.to_sql(table_name, con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_wiki(filtered_urls, 'wiki_scrape', engine)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
